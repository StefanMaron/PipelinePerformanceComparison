name: Full Pipeline Comparison

on:
  workflow_dispatch:
    inputs:
      description:
        description: 'Comparison run description (optional)'
        required: false
        type: string

env:
  AL_VERSION: 'latest'
  BC_VERSION: '26.5'
  CONTAINER_NAME: 'bcserver'
  BCDEV_REPO: 'https://github.com/StefanMaron/BCDevOnLinux.git'
  BCDEV_BRANCH: 'v2'

jobs:
  # ============================================================================
  # BUILD JOBS - Run in parallel
  # ============================================================================

  linux-full-test:
    uses: ./.github/workflows/build-linux.yml
    name: Linux Full Test

  linux-compile-only:
    uses: ./.github/workflows/build-linux-compile-only.yml
    name: Linux Compile-Only

  windows-full-test:
    uses: ./.github/workflows/build-windows.yml
    name: Windows Full Test

  windows-compile-only:
    uses: ./.github/workflows/build-windows-compile-only.yml
    name: Windows Compile-Only

  # ============================================================================
  # ANALYSIS JOB - Runs after all builds complete
  # ============================================================================

  analyze-results:
    runs-on: ubuntu-latest
    name: Analyze & Compare Results
    needs: [linux-full-test, linux-compile-only, windows-full-test, windows-compile-only]
    if: always() # Run even if some builds fail

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install pandas tabulate

    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: ./metrics

    - name: List downloaded artifacts
      run: |
        echo "=== Downloaded Artifacts ==="
        find ./metrics -type f -name "*.json" | sort
        echo ""

    - name: Fetch GitHub Actions job timings
      run: |
        echo "Fetching actual GitHub Actions job timings from current run..."

        # Get job durations from the current workflow run
        CURRENT_RUN_ID="${{ github.run_id }}"

        # Function to get job duration
        get_job_duration() {
          local job_name=$1
          gh api repos/${{ github.repository }}/actions/runs/$CURRENT_RUN_ID/jobs --jq ".jobs[] | select(.name == \"$job_name\") | {started_at, completed_at, duration_seconds: (((.completed_at | fromdateiso8601) - (.started_at | fromdateiso8601)))}" 2>/dev/null || echo "{}"
        }

        # Fetch job durations
        mkdir -p ./metrics/github_timings

        get_job_duration "Linux Full Test" > ./metrics/github_timings/linux-full-test.json
        get_job_duration "Linux Compile-Only" > ./metrics/github_timings/linux-compile-only.json
        get_job_duration "Windows Full Test" > ./metrics/github_timings/windows-full-test.json
        get_job_duration "Windows Compile-Only" > ./metrics/github_timings/windows-compile-only.json

        echo "✓ GitHub Actions timings fetched"
        ls -la ./metrics/github_timings/
      env:
        GH_TOKEN: ${{ github.token }}

    - name: Create metrics analysis script
      run: |
        cat > analyze_metrics.py << 'EOF'
        import json
        import pandas as pd
        from pathlib import Path
        from tabulate import tabulate
        from datetime import datetime

        # Load all metrics files
        metrics_dir = Path('./metrics')

        # Find all JSON metrics files recursively
        metrics_files = {
            'Linux Full Test': 'linux-full-test-metrics.json',
            'Linux Compile-Only': 'linux-compile-only-metrics.json',
            'Windows Full Test': 'windows-full-test-metrics.json',
            'Windows Compile-Only': 'windows-compile-only-metrics.json'
        }

        data = {}
        for pipeline_name, filename in metrics_files.items():
            # Search recursively for the file
            found_files = list(metrics_dir.rglob(filename))
            if found_files:
                file_path = found_files[0]
                try:
                    # Handle UTF-8 BOM that Windows PowerShell might add
                    with open(file_path, 'r', encoding='utf-8-sig') as f:
                        data[pipeline_name] = json.load(f)
                    print(f"✓ Loaded {pipeline_name} from {file_path}")
                except Exception as e:
                    print(f"✗ Error loading {pipeline_name}: {e}")
            else:
                print(f"✗ Missing {pipeline_name} - {filename}")

        if not data:
            print("ERROR: No metrics files found!")
            exit(1)

        # Load GitHub Actions timings
        github_timings = {}
        github_timing_files = {
            'Linux Full Test': 'github_timings/linux-full-test.json',
            'Linux Compile-Only': 'github_timings/linux-compile-only.json',
            'Windows Full Test': 'github_timings/windows-full-test.json',
            'Windows Compile-Only': 'github_timings/windows-compile-only.json'
        }

        for pipeline_name, filename in github_timing_files.items():
            file_path = metrics_dir / filename
            if file_path.exists():
                try:
                    with open(file_path, 'r') as f:
                        timing = json.load(f)
                        if timing.get('duration_seconds'):
                            github_timings[pipeline_name] = timing['duration_seconds']
                            print(f"✓ Loaded GitHub timing for {pipeline_name}")
                except Exception as e:
                    print(f"✗ Error loading GitHub timing for {pipeline_name}: {e}")

        print(f"\n✓ Successfully loaded {len(data)} pipeline(s)\n")

        # Create comparison dataframe
        comparison_data = []

        # Core comparable metrics (all pipelines)
        core_metrics = [
            ('Setup (.NET + Tools)', 'setup_duration'),
            ('Directory Creation', 'dir_creation_duration'),
            ('Dependencies Download', 'total_dependency_duration'),
            ('Pre-compile Setup', 'precompile_setup_duration'),
            ('Compilation', 'compile_duration'),
            ('Post-compile Analysis', 'post_compile_analysis_duration'),
        ]

        # Full test only metrics
        full_test_metrics = [
            ('Container Setup', ['container_creation_duration', 'docker_setup_duration', 'bcdev_clone_duration',
                                 'container_build_duration', 'container_start_duration']),
            ('App Publishing', 'publish_duration'),
            ('Test Execution', 'test_duration'),
            ('Cleanup', 'cleanup_duration'),
        ]

        # Build comparison rows
        for metric_name, metric_keys in core_metrics:
            row = {'Phase': metric_name}
            for pipeline_name, metrics in data.items():
                if isinstance(metric_keys, str):
                    value = metrics.get(metric_keys, 0)
                else:
                    value = sum(metrics.get(k, 0) for k in metric_keys)
                row[pipeline_name] = f"{value:.2f}s"
            comparison_data.append(row)

        # Add full test metrics
        for metric_name, metric_keys in full_test_metrics:
            row = {'Phase': metric_name}
            for pipeline_name, metrics in data.items():
                if 'Full Test' in pipeline_name:
                    if isinstance(metric_keys, str):
                        value = metrics.get(metric_keys, 0)
                    else:
                        value = sum(metrics.get(k, 0) for k in metric_keys)
                    row[pipeline_name] = f"{value:.2f}s"
                else:
                    row[pipeline_name] = "N/A"
            comparison_data.append(row)

        # Add separator
        comparison_data.append({
            'Phase': '─' * 30,
            **{name: '─' * 10 for name in data.keys()}
        })

        # Add total time (measured)
        total_row = {'Phase': '**TOTAL BUILD TIME (Measured)**'}
        for pipeline_name, metrics in data.items():
            total_row[pipeline_name] = f"**{metrics.get('total_duration', 0):.2f}s**"
        comparison_data.append(total_row)

        # Add GitHub Actions total time
        if github_timings:
            github_row = {'Phase': 'GitHub Actions Total Time'}
            for pipeline_name in data.keys():
                if pipeline_name in github_timings:
                    github_row[pipeline_name] = f"{github_timings[pipeline_name]:.2f}s"
                else:
                    github_row[pipeline_name] = "N/A"
            comparison_data.append(github_row)

            # Add overhead calculation
            overhead_row = {'Phase': 'Runner Overhead (Setup+Teardown)'}
            for pipeline_name, metrics in data.items():
                if pipeline_name in github_timings:
                    measured = metrics.get('total_duration', 0)
                    github_total = github_timings[pipeline_name]
                    overhead = github_total - measured
                    overhead_pct = (overhead / github_total * 100) if github_total > 0 else 0
                    overhead_row[pipeline_name] = f"{overhead:.2f}s ({overhead_pct:.1f}%)"
                else:
                    overhead_row[pipeline_name] = "N/A"
            comparison_data.append(overhead_row)

        # Add artifacts info
        comparison_data.append({
            'Phase': '',
            **{name: '' for name in data.keys()}
        })
        comparison_data.append({
            'Phase': 'Apps Generated',
            **{name: str(metrics.get('app_count', 0)) for name, metrics in data.items()}
        })
        comparison_data.append({
            'Phase': 'Total App Size',
            **{name: f"{metrics.get('total_app_size_kb', 0):.2f} KB" for name, metrics in data.items()}
        })

        # Create DataFrame
        df = pd.DataFrame(comparison_data)

        # Generate Markdown table
        print("\n" + "="*100)
        print("PERFORMANCE COMPARISON TABLE")
        print("="*100 + "\n")

        markdown_table = tabulate(df, headers='keys', tablefmt='github', showindex=False)
        print(markdown_table)

        # Save Markdown table
        with open('performance-comparison.md', 'w') as f:
            f.write("# Pipeline Performance Comparison\n\n")
            f.write(f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}*\n\n")
            f.write(markdown_table)
            f.write("\n\n## Key Findings\n\n")

            # Calculate speedups - Platform comparison first
            if 'Linux Compile-Only' in data and 'Windows Compile-Only' in data:
                linux_time = data['Linux Compile-Only']['total_duration']
                windows_time = data['Windows Compile-Only']['total_duration']
                diff = ((windows_time - linux_time) / linux_time * 100)
                if diff > 0:
                    f.write(f"- **Linux vs Windows** (Compile-only): Linux is **{diff:.1f}% faster** than Windows ({linux_time:.2f}s vs {windows_time:.2f}s)\n")
                else:
                    f.write(f"- **Linux vs Windows** (Compile-only): Windows is **{abs(diff):.1f}% faster** than Linux ({windows_time:.2f}s vs {linux_time:.2f}s)\n")

            if 'Linux Full Test' in data and 'Windows Full Test' in data:
                linux_time = data['Linux Full Test']['total_duration']
                windows_time = data['Windows Full Test']['total_duration']
                diff = ((windows_time - linux_time) / linux_time * 100)
                if diff > 0:
                    f.write(f"- **Linux vs Windows** (Full test): Linux is **{diff:.1f}% faster** than Windows ({linux_time:.2f}s vs {windows_time:.2f}s)\n")
                else:
                    f.write(f"- **Linux vs Windows** (Full test): Windows is **{abs(diff):.1f}% faster** than Linux ({windows_time:.2f}s vs {linux_time:.2f}s)\n")

            if 'Linux Compile-Only' in data and 'Linux Full Test' in data:
                compile_time = data['Linux Compile-Only']['total_duration']
                full_time = data['Linux Full Test']['total_duration']
                speedup = (full_time - compile_time) / compile_time * 100
                f.write(f"- **Linux**: Compile-only is {abs(speedup):.1f}% faster than full test\n")

            if 'Windows Compile-Only' in data and 'Windows Full Test' in data:
                compile_time = data['Windows Compile-Only']['total_duration']
                full_time = data['Windows Full Test']['total_duration']
                speedup = (full_time - compile_time) / compile_time * 100
                f.write(f"- **Windows**: Compile-only is {abs(speedup):.1f}% faster than full test\n")

        print("\n✓ Saved to performance-comparison.md")

        # Generate CSV for Excel/PowerPoint
        df.to_csv('performance-comparison.csv', index=False)
        print("✓ Saved to performance-comparison.csv")

        print("\n" + "="*100)
        print("Analysis complete! Files generated:")
        print("  - performance-comparison.md   (Markdown table)")
        print("  - performance-comparison.csv  (Excel/PowerPoint ready)")
        print("="*100)
        EOF

    - name: Run analysis
      run: python analyze_metrics.py

    - name: Display results in summary
      if: always()
      run: |
        if [ -f performance-comparison.md ]; then
          echo "## 📊 Performance Comparison Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat performance-comparison.md >> $GITHUB_STEP_SUMMARY
        fi

    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: performance-analysis
        path: |
          performance-comparison.md
          performance-comparison.csv
        retention-days: 90
