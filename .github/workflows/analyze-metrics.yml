name: Analyze Performance Metrics

on:
  workflow_dispatch:
    inputs:
      run_id:
        description: 'GitHub Run ID to analyze (leave empty for latest)'
        required: false
        type: string

jobs:
  analyze-metrics:
    runs-on: ubuntu-latest
    name: Generate Performance Comparison Table

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install pandas tabulate

    - name: Download Linux Full Test artifacts
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: build-linux.yml
        name: linux-full-test-artifacts
        path: ./metrics
        if_no_artifact_found: warn

    - name: Download Linux Compile-Only artifacts
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: build-linux-compile-only.yml
        name: linux-compile-only-artifacts
        path: ./metrics
        if_no_artifact_found: warn

    - name: Download Windows Full Test artifacts
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: build-windows.yml
        name: windows-full-test-artifacts
        path: ./metrics
        if_no_artifact_found: warn

    - name: Download Windows Compile-Only artifacts
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: build-windows-compile-only.yml
        name: windows-compile-only-artifacts
        path: ./metrics
        if_no_artifact_found: warn

    - name: Fetch GitHub Actions job timings
      run: |
        echo "Fetching actual GitHub Actions job timings..."

        # Get the run IDs from the downloaded metrics
        LINUX_FULL_RUN_ID=$(jq -r '.github_run_id // empty' ./metrics/linux-full-test-metrics.json 2>/dev/null || echo "")
        LINUX_COMPILE_RUN_ID=$(jq -r '.github_run_id // empty' ./metrics/linux-compile-only-metrics.json 2>/dev/null || echo "")
        WINDOWS_FULL_RUN_ID=$(jq -r '.github_run_id // empty' ./metrics/windows-full-test-metrics.json 2>/dev/null || echo "")
        WINDOWS_COMPILE_RUN_ID=$(jq -r '.github_run_id // empty' ./metrics/windows-compile-only-metrics.json 2>/dev/null || echo "")

        # Function to get job duration
        get_job_duration() {
          local run_id=$1
          local job_name=$2
          if [ -n "$run_id" ]; then
            gh api repos/${{ github.repository }}/actions/runs/$run_id/jobs --jq ".jobs[] | select(.name == \"$job_name\") | {started_at, completed_at, duration_seconds: (((.completed_at | fromdateiso8601) - (.started_at | fromdateiso8601)))}" 2>/dev/null || echo "{}"
          else
            echo "{}"
          fi
        }

        # Fetch job durations
        mkdir -p ./metrics/github_timings

        if [ -n "$LINUX_FULL_RUN_ID" ]; then
          get_job_duration "$LINUX_FULL_RUN_ID" "Linux Full Test Build" > ./metrics/github_timings/linux-full-test.json
        fi

        if [ -n "$LINUX_COMPILE_RUN_ID" ]; then
          get_job_duration "$LINUX_COMPILE_RUN_ID" "Linux Compile-Only Build" > ./metrics/github_timings/linux-compile-only.json
        fi

        if [ -n "$WINDOWS_FULL_RUN_ID" ]; then
          get_job_duration "$WINDOWS_FULL_RUN_ID" "Windows Full Test Build" > ./metrics/github_timings/windows-full-test.json
        fi

        if [ -n "$WINDOWS_COMPILE_RUN_ID" ]; then
          get_job_duration "$WINDOWS_COMPILE_RUN_ID" "Windows Compile-Only Build" > ./metrics/github_timings/windows-compile-only.json
        fi

        echo "✓ GitHub Actions timings fetched"
      env:
        GH_TOKEN: ${{ github.token }}

    - name: Create metrics analysis script
      run: |
        cat > analyze_metrics.py << 'EOF'
        import json
        import pandas as pd
        from pathlib import Path
        from tabulate import tabulate
        from datetime import datetime

        # Load all metrics files
        metrics_dir = Path('./metrics')
        # Order determines column order in output table
        metrics_files = {
            'Linux Compile-Only': 'linux-compile-only-metrics.json',
            'Linux Full Test': 'linux-full-test-metrics.json',
            'Windows Compile-Only': 'windows-compile-only-metrics.json',
            'Windows Full Test': 'windows-full-test-metrics.json'
        }

        data = {}
        for pipeline_name, filename in metrics_files.items():
            file_path = metrics_dir / filename
            if file_path.exists():
                try:
                    # Handle UTF-8 BOM that Windows PowerShell might add
                    with open(file_path, 'r', encoding='utf-8-sig') as f:
                        data[pipeline_name] = json.load(f)
                    print(f"✓ Loaded {pipeline_name}")
                except Exception as e:
                    print(f"✗ Error loading {pipeline_name}: {e}")
            else:
                print(f"✗ Missing {pipeline_name} - {filename}")

        if not data:
            print("ERROR: No metrics files found!")
            exit(1)

        # Load GitHub Actions timings
        github_timings = {}
        github_timing_files = {
            'Linux Compile-Only': 'github_timings/linux-compile-only.json',
            'Linux Full Test': 'github_timings/linux-full-test.json',
            'Windows Compile-Only': 'github_timings/windows-compile-only.json',
            'Windows Full Test': 'github_timings/windows-full-test.json'
        }

        for pipeline_name, filename in github_timing_files.items():
            file_path = metrics_dir / filename
            if file_path.exists():
                try:
                    with open(file_path, 'r') as f:
                        timing = json.load(f)
                        if timing.get('duration_seconds'):
                            github_timings[pipeline_name] = timing['duration_seconds']
                            print(f"✓ Loaded GitHub timing for {pipeline_name}")
                except Exception as e:
                    print(f"✗ Error loading GitHub timing for {pipeline_name}: {e}")

        print(f"\n✓ Successfully loaded {len(data)} pipeline(s)\n")

        # Create comparison dataframe
        comparison_data = []

        # Core comparable metrics (all pipelines)
        core_metrics = [
            ('Setup (.NET + Tools)', 'setup_duration'),
            ('Directory Creation', 'dir_creation_duration'),
            ('Dependencies Download', 'total_dependency_duration'),
            ('Pre-compile Setup', 'precompile_setup_duration'),
            ('Compilation', 'compile_duration'),
            ('Post-compile Analysis', 'post_compile_analysis_duration'),
        ]

        # Full test only metrics
        full_test_metrics = [
            ('Container Setup', ['container_creation_duration', 'docker_setup_duration', 'bcdev_clone_duration',
                                 'container_build_duration', 'container_start_duration']),
            ('App Publishing', 'publish_duration'),
            ('Test Execution', 'test_duration'),
            ('Cleanup', 'cleanup_duration'),
        ]

        # Build comparison rows
        for metric_name, metric_keys in core_metrics:
            row = {'Phase': metric_name}
            for pipeline_name, metrics in data.items():
                if isinstance(metric_keys, str):
                    value = metrics.get(metric_keys, 0)
                else:
                    value = sum(metrics.get(k, 0) for k in metric_keys)
                row[pipeline_name] = f"{value:.2f}s"
            comparison_data.append(row)

        # Add full test metrics
        for metric_name, metric_keys in full_test_metrics:
            row = {'Phase': metric_name}
            for pipeline_name, metrics in data.items():
                if 'Full Test' in pipeline_name:
                    if isinstance(metric_keys, str):
                        value = metrics.get(metric_keys, 0)
                    else:
                        value = sum(metrics.get(k, 0) for k in metric_keys)
                    row[pipeline_name] = f"{value:.2f}s"
                else:
                    row[pipeline_name] = "N/A"
            comparison_data.append(row)

        # Add separator
        comparison_data.append({
            'Phase': '─' * 30,
            **{name: '─' * 10 for name in data.keys()}
        })

        # Add total time (measured)
        total_row = {'Phase': '**TOTAL BUILD TIME (Measured)**'}
        for pipeline_name, metrics in data.items():
            total_row[pipeline_name] = f"**{metrics.get('total_duration', 0):.2f}s**"
        comparison_data.append(total_row)

        # Add GitHub Actions total time
        if github_timings:
            github_row = {'Phase': 'GitHub Actions Total Time'}
            for pipeline_name in data.keys():
                if pipeline_name in github_timings:
                    github_row[pipeline_name] = f"{github_timings[pipeline_name]:.2f}s"
                else:
                    github_row[pipeline_name] = "N/A"
            comparison_data.append(github_row)

            # Add overhead calculation
            overhead_row = {'Phase': 'Runner Overhead (Setup+Teardown)'}
            for pipeline_name, metrics in data.items():
                if pipeline_name in github_timings:
                    measured = metrics.get('total_duration', 0)
                    github_total = github_timings[pipeline_name]
                    overhead = github_total - measured
                    overhead_pct = (overhead / github_total * 100) if github_total > 0 else 0
                    overhead_row[pipeline_name] = f"{overhead:.2f}s ({overhead_pct:.1f}%)"
                else:
                    overhead_row[pipeline_name] = "N/A"
            comparison_data.append(overhead_row)

        # Add artifacts info
        comparison_data.append({
            'Phase': '',
            **{name: '' for name in data.keys()}
        })

        # Apps Generated - show N/A for compile-only pipelines
        apps_row = {'Phase': 'Apps Generated'}
        for name, metrics in data.items():
            if 'Compile-Only' in name:
                apps_row[name] = 'N/A'
            else:
                apps_row[name] = str(metrics.get('app_count', 0))
        comparison_data.append(apps_row)

        # Total App Size - show N/A for compile-only pipelines
        size_row = {'Phase': 'Total App Size'}
        for name, metrics in data.items():
            if 'Compile-Only' in name:
                size_row[name] = 'N/A'
            else:
                size_row[name] = f"{metrics.get('total_app_size_kb', 0):.2f} KB"
        comparison_data.append(size_row)

        # Create DataFrame with explicit column order
        column_order = ['Phase', 'Linux Compile-Only', 'Linux Full Test', 'Windows Compile-Only', 'Windows Full Test']
        # Filter to only include columns that exist in the data
        available_columns = ['Phase'] + [col for col in column_order[1:] if col in data.keys()]
        df = pd.DataFrame(comparison_data)
        df = df[available_columns]

        # Generate Markdown table
        print("\n" + "="*100)
        print("PERFORMANCE COMPARISON TABLE")
        print("="*100 + "\n")

        markdown_table = tabulate(df, headers='keys', tablefmt='github', showindex=False)
        print(markdown_table)

        # Save Markdown table
        with open('performance-comparison.md', 'w') as f:
            f.write("# Pipeline Performance Comparison\n\n")
            f.write(f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}*\n\n")
            f.write(markdown_table)
            f.write("\n\n## Key Findings\n\n")

            # Calculate speedups - Platform comparison first
            if 'Linux Compile-Only' in data and 'Windows Compile-Only' in data:
                linux_time = data['Linux Compile-Only']['total_duration']
                windows_time = data['Windows Compile-Only']['total_duration']
                diff = ((windows_time - linux_time) / linux_time * 100)
                if diff > 0:
                    f.write(f"- **Linux vs Windows** (Compile-only): Linux is **{diff:.1f}% faster** than Windows ({linux_time:.2f}s vs {windows_time:.2f}s)\n")
                else:
                    f.write(f"- **Linux vs Windows** (Compile-only): Windows is **{abs(diff):.1f}% faster** than Linux ({windows_time:.2f}s vs {linux_time:.2f}s)\n")

            if 'Linux Full Test' in data and 'Windows Full Test' in data:
                linux_time = data['Linux Full Test']['total_duration']
                windows_time = data['Windows Full Test']['total_duration']
                diff = ((windows_time - linux_time) / linux_time * 100)
                if diff > 0:
                    f.write(f"- **Linux vs Windows** (Full test): Linux is **{diff:.1f}% faster** than Windows ({linux_time:.2f}s vs {windows_time:.2f}s)\n")
                else:
                    f.write(f"- **Linux vs Windows** (Full test): Windows is **{abs(diff):.1f}% faster** than Linux ({windows_time:.2f}s vs {linux_time:.2f}s)\n")

            if 'Linux Compile-Only' in data and 'Linux Full Test' in data:
                compile_time = data['Linux Compile-Only']['total_duration']
                full_time = data['Linux Full Test']['total_duration']
                speedup = (full_time - compile_time) / compile_time * 100
                f.write(f"- **Linux**: Compile-only is {abs(speedup):.1f}% faster than full test\n")

            if 'Windows Compile-Only' in data and 'Windows Full Test' in data:
                compile_time = data['Windows Compile-Only']['total_duration']
                full_time = data['Windows Full Test']['total_duration']
                speedup = (full_time - compile_time) / compile_time * 100
                f.write(f"- **Windows**: Compile-only is {abs(speedup):.1f}% faster than full test\n")

        print("\n✓ Saved to performance-comparison.md")

        # Generate CSV for Excel/PowerPoint
        df.to_csv('performance-comparison.csv', index=False)
        print("✓ Saved to performance-comparison.csv")

        print("\n" + "="*100)
        print("Analysis complete! Files generated:")
        print("  - performance-comparison.md   (Markdown table)")
        print("  - performance-comparison.csv  (Excel/PowerPoint ready)")
        print("="*100)
        EOF

    - name: Run analysis
      run: python analyze_metrics.py

    - name: Display results
      if: always()
      run: |
        if [ -f performance-comparison.md ]; then
          echo "=== PERFORMANCE COMPARISON TABLE ==="
          cat performance-comparison.md
        fi

    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: performance-analysis
        path: |
          performance-comparison.md
          performance-comparison.csv
        retention-days: 90
