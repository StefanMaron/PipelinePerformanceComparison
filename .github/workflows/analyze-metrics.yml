name: Analyze Performance Metrics

on:
  workflow_dispatch:
    inputs:
      run_id:
        description: 'GitHub Run ID to analyze (leave empty for latest)'
        required: false
        type: string

jobs:
  analyze-metrics:
    runs-on: ubuntu-latest
    name: Generate Performance Comparison Table

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install pandas tabulate matplotlib pillow

    - name: Download Linux Full Test artifacts
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: build-linux.yml
        name: linux-full-test-artifacts
        path: ./metrics
        if_no_artifact_found: warn

    - name: Download Linux Compile-Only artifacts
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: build-linux-compile-only.yml
        name: linux-compile-only-artifacts
        path: ./metrics
        if_no_artifact_found: warn

    - name: Download Windows Full Test artifacts
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: build-windows.yml
        name: windows-full-test-artifacts
        path: ./metrics
        if_no_artifact_found: warn

    - name: Download Windows Compile-Only artifacts
      uses: dawidd6/action-download-artifact@v3
      with:
        workflow: build-windows-compile-only.yml
        name: windows-compile-only-artifacts
        path: ./metrics
        if_no_artifact_found: warn

    - name: Create metrics analysis script
      run: |
        cat > analyze_metrics.py << 'EOF'
        import json
        import pandas as pd
        from pathlib import Path
        from tabulate import tabulate
        import matplotlib.pyplot as plt
        import matplotlib.patches as mpatches
        from datetime import datetime

        # Load all metrics files
        metrics_dir = Path('./metrics')
        metrics_files = {
            'Linux Full Test': 'linux-full-test-metrics.json',
            'Linux Compile-Only': 'linux-compile-only-metrics.json',
            'Windows Full Test': 'windows-full-test-metrics.json',
            'Windows Compile-Only': 'windows-compile-only-metrics.json'
        }

        data = {}
        for pipeline_name, filename in metrics_files.items():
            file_path = metrics_dir / filename
            if file_path.exists():
                with open(file_path, 'r') as f:
                    data[pipeline_name] = json.load(f)
                print(f"✓ Loaded {pipeline_name}")
            else:
                print(f"✗ Missing {pipeline_name} - {filename}")

        if not data:
            print("ERROR: No metrics files found!")
            exit(1)

        # Create comparison dataframe
        comparison_data = []

        # Core comparable metrics (all pipelines)
        core_metrics = [
            ('Setup (.NET + Tools)', 'setup_duration'),
            ('Directory Creation', 'dir_creation_duration'),
            ('Dependencies Download', 'total_dependency_duration'),
            ('Pre-compile Setup', 'precompile_setup_duration'),
            ('Compilation', 'compile_duration'),
            ('Post-compile Analysis', 'post_compile_analysis_duration'),
        ]

        # Full test only metrics
        full_test_metrics = [
            ('Container Setup', ['container_creation_duration', 'docker_setup_duration', 'bcdev_clone_duration',
                                 'container_build_duration', 'container_start_duration']),
            ('App Publishing', 'publish_duration'),
            ('Test Execution', 'test_duration'),
            ('Cleanup', 'cleanup_duration'),
        ]

        # Build comparison rows
        for metric_name, metric_keys in core_metrics:
            row = {'Phase': metric_name}
            for pipeline_name, metrics in data.items():
                if isinstance(metric_keys, str):
                    value = metrics.get(metric_keys, 0)
                else:
                    value = sum(metrics.get(k, 0) for k in metric_keys)
                row[pipeline_name] = f"{value:.2f}s"
            comparison_data.append(row)

        # Add full test metrics
        for metric_name, metric_keys in full_test_metrics:
            row = {'Phase': metric_name}
            for pipeline_name, metrics in data.items():
                if 'Full Test' in pipeline_name:
                    if isinstance(metric_keys, str):
                        value = metrics.get(metric_keys, 0)
                    else:
                        value = sum(metrics.get(k, 0) for k in metric_keys)
                    row[pipeline_name] = f"{value:.2f}s"
                else:
                    row[pipeline_name] = "N/A"
            comparison_data.append(row)

        # Add separator
        comparison_data.append({
            'Phase': '─' * 30,
            **{name: '─' * 10 for name in data.keys()}
        })

        # Add total time
        total_row = {'Phase': '**TOTAL BUILD TIME**'}
        for pipeline_name, metrics in data.items():
            total_row[pipeline_name] = f"**{metrics.get('total_duration', 0):.2f}s**"
        comparison_data.append(total_row)

        # Add artifacts info
        comparison_data.append({
            'Phase': '',
            **{name: '' for name in data.keys()}
        })
        comparison_data.append({
            'Phase': 'Apps Generated',
            **{name: str(metrics.get('app_count', 0)) for name, metrics in data.items()}
        })
        comparison_data.append({
            'Phase': 'Total App Size',
            **{name: f"{metrics.get('total_app_size_kb', 0):.2f} KB" for name, metrics in data.items()}
        })

        # Create DataFrame
        df = pd.DataFrame(comparison_data)

        # Generate Markdown table
        print("\n" + "="*100)
        print("PERFORMANCE COMPARISON TABLE")
        print("="*100 + "\n")

        markdown_table = tabulate(df, headers='keys', tablefmt='github', showindex=False)
        print(markdown_table)

        # Save Markdown table
        with open('performance-comparison.md', 'w') as f:
            f.write("# Pipeline Performance Comparison\n\n")
            f.write(f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}*\n\n")
            f.write(markdown_table)
            f.write("\n\n## Key Findings\n\n")

            # Calculate speedups
            if 'Linux Compile-Only' in data and 'Linux Full Test' in data:
                compile_time = data['Linux Compile-Only']['total_duration']
                full_time = data['Linux Full Test']['total_duration']
                speedup = (full_time - compile_time) / compile_time * 100
                f.write(f"- **Linux**: Compile-only is {abs(speedup):.1f}% faster than full test\n")

            if 'Windows Compile-Only' in data and 'Windows Full Test' in data:
                compile_time = data['Windows Compile-Only']['total_duration']
                full_time = data['Windows Full Test']['total_duration']
                speedup = (full_time - compile_time) / compile_time * 100
                f.write(f"- **Windows**: Compile-only is {abs(speedup):.1f}% faster than full test\n")

            if 'Linux Full Test' in data and 'Windows Full Test' in data:
                linux_time = data['Linux Full Test']['total_duration']
                windows_time = data['Windows Full Test']['total_duration']
                diff = ((windows_time - linux_time) / linux_time * 100)
                if diff > 0:
                    f.write(f"- **Platform**: Windows full test is {diff:.1f}% slower than Linux\n")
                else:
                    f.write(f"- **Platform**: Windows full test is {abs(diff):.1f}% faster than Linux\n")

        print("\n✓ Saved to performance-comparison.md")

        # Generate CSV for Excel/PowerPoint
        df.to_csv('performance-comparison.csv', index=False)
        print("✓ Saved to performance-comparison.csv")

        # Generate visual chart
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # Chart 1: Total Build Time Comparison
        pipeline_names = list(data.keys())
        total_times = [metrics['total_duration'] for metrics in data.values()]
        colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']

        bars1 = ax1.bar(range(len(pipeline_names)), total_times, color=colors, edgecolor='black', linewidth=1.5)
        ax1.set_xlabel('Pipeline', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')
        ax1.set_title('Total Build Time Comparison', fontsize=14, fontweight='bold', pad=20)
        ax1.set_xticks(range(len(pipeline_names)))
        ax1.set_xticklabels([name.replace(' ', '\n') for name in pipeline_names], fontsize=10)
        ax1.grid(axis='y', alpha=0.3, linestyle='--')

        # Add value labels on bars
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}s',
                    ha='center', va='bottom', fontweight='bold', fontsize=11)

        # Chart 2: Phase Breakdown (Stacked Bar)
        phases = {
            'Setup': 'setup_duration',
            'Dependencies': 'total_dependency_duration',
            'Compilation': 'compile_duration',
            'Container/Test': ['container_creation_duration', 'docker_setup_duration',
                              'bcdev_clone_duration', 'container_build_duration',
                              'container_start_duration', 'publish_duration', 'test_duration']
        }

        phase_data = {phase: [] for phase in phases.keys()}
        for pipeline_name in pipeline_names:
            metrics = data[pipeline_name]
            for phase, keys in phases.items():
                if isinstance(keys, str):
                    value = metrics.get(keys, 0)
                else:
                    value = sum(metrics.get(k, 0) for k in keys)
                phase_data[phase].append(value)

        bottom = [0] * len(pipeline_names)
        phase_colors = ['#6A994E', '#BC4749', '#F2CC8F', '#81B29A']

        for idx, (phase, values) in enumerate(phase_data.items()):
            ax2.bar(range(len(pipeline_names)), values, bottom=bottom,
                   label=phase, color=phase_colors[idx], edgecolor='white', linewidth=1)
            bottom = [b + v for b, v in zip(bottom, values)]

        ax2.set_xlabel('Pipeline', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')
        ax2.set_title('Build Time Phase Breakdown', fontsize=14, fontweight='bold', pad=20)
        ax2.set_xticks(range(len(pipeline_names)))
        ax2.set_xticklabels([name.replace(' ', '\n') for name in pipeline_names], fontsize=10)
        ax2.legend(loc='upper left', fontsize=10, framealpha=0.9)
        ax2.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig('performance-comparison.png', dpi=300, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        print("✓ Saved to performance-comparison.png")

        print("\n" + "="*100)
        print("Analysis complete! Files generated:")
        print("  - performance-comparison.md   (Markdown table)")
        print("  - performance-comparison.csv  (Excel/PowerPoint ready)")
        print("  - performance-comparison.png  (High-res charts)")
        print("="*100)
        EOF

    - name: Run analysis
      run: python analyze_metrics.py

    - name: Display results
      if: always()
      run: |
        if [ -f performance-comparison.md ]; then
          echo "=== PERFORMANCE COMPARISON TABLE ==="
          cat performance-comparison.md
        fi

    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: performance-analysis
        path: |
          performance-comparison.md
          performance-comparison.csv
          performance-comparison.png
        retention-days: 90
